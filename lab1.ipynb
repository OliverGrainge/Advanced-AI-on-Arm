{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20addd1f-b4bb-41a9-b4b0-58843a6807c3",
   "metadata": {},
   "source": [
    "# Lab 1: Extreme Quantization\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Deep neural networks are often large and computationally expensive, especially when trained and stored in floating-point format. Although 8-bit quantization has become an industry-standard approach for reducing model size and improving efficiency, it has been shown in research that even lower-bit formats can show an increased accuracy-to-memory trade-off. This leads us into the realm of **extreme quantization**, where parameters (and sometimes activations) are quantized to 4 bits, 2 bits, or even a single bit (binary networks).\n",
    "\n",
    "By pushing below 8 bits, the potential gains in memory footprint and inference speed can be enormous. For instance, a binary network may require as little as 1 bit per weight, cutting the size by a factor of 32 compared to a typical FP32 model. The computational speedups come from the possibility of using bitwise operations (like XNOR and popcount) or small lookup tables (LUTs) for matrix multiplications, which can outperform standard floating-point arithmetic on many Arm-based systems. However, accuracy often suffers without special training or architectural modifications, and not all libraries offer robust support for sub-8-bit kernels.\n",
    "\n",
    "This lab offers a hands-on exploration of these trade-offs. We will begin by training baseline floating-point (FP32) model, then progressively quantize it to 8 bits, 4 bits, 2 bits, and finally 1 bit. Along the way, you will see how each step impacts model size, computational efficiency, and accuracy. Although we will not implement custom kernels in this lab, we will discuss how bitwise or LUT-based operations achieve speed gains on modern hardware. Our ultimate goal is to develop an intuition for when and why extreme quantization is useful, and to motivate more advanced techniques‚Äîsuch as quantization-aware training‚Äîin subsequent labs and inspire you to develope, new and imprved extremely quantized models\n",
    "\n",
    "### Lab Objectives\n",
    "\n",
    "1. Demonstrate the practical impact of going from 8-bit down to 1-bit quantization in terms of model size, speed, and accuracy.  \n",
    "2. Discuss how bitwise operations or lookup tables (LUTs) offer significant performance advantages for ultra-low-precision arithmetic.  \n",
    "3. Identify common pitfalls of extreme quantization and why accuracy degrades sharply without specialized training methods.  \n",
    "4. Lay the groundwork for future labs on quantization-aware training and custom operator design.\n",
    "\n",
    "By the end of this lab, you will have a clearer picture of the benefits and limitations of extreme quantization, helping you understand how to balance efficiency and performance in real-world deployments on resource-constrained Arm devices.\n",
    "\n",
    "---\n",
    "\n",
    "## Training your own Vision Transformer\n",
    "\n",
    "lets start by training your own vision transformer on the popular cifar10 dataset. Once we have done so we can play with quantizing the model, post-training. This is a popular technique as, post-training quantization avoids the need to retrain the model, avoiding a computationally expensive step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791e4852-d8bc-4fd8-b98d-42e6710a4a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds.shape torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "vit = ViT(\n",
    "    image_size = 32,     # CIFAR-10 images are 32x32\n",
    "    patch_size = 4,      # The size of the image path split into a single token\n",
    "    num_classes = 10,    # The number of classes in the dataset\n",
    "    dim = 512,           # The inner dimension of the model (number of channels in the tokens)\n",
    "    depth = 6,           # The number of transformer blocks\n",
    "    heads = 8,           # The number of attnetion heads per transformer block \n",
    "    mlp_dim = 1024,      # The feedforward dimension of the MLP\n",
    "    dropout = 0.1,       # The dropout rate for the transformer\n",
    "    emb_dropout = 0.1    # The dropout rate for the embedding\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "preds = vit(img) \n",
    "print(\"preds.shape\", preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812c9b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | ViT  | 12.7 M | train\n",
      "---------------------------------------\n",
      "12.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.7 M    Total params\n",
      "50.695    Total estimated model params size (MB)\n",
      "114       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 69.9M/170M [00:55<01:20, 1.25MB/s]\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:120\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;129m@_no_grad_context\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[_OUT_DICT]:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:176\u001b[39m, in \u001b[36m_EvaluationLoop.setup_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m source = \u001b[38;5;28mself\u001b[39m._data_source\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m dataloaders = \u001b[43m_request_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m trainer.strategy.barrier(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage.dataloader_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dataloader()\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:326\u001b[39m, in \u001b[36m_request_dataloader\u001b[39m\u001b[34m(data_source)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_source\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:290\u001b[39m, in \u001b[36m_DataLoaderSource.dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.instance, pl.LightningModule):\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.instance, pl.LightningDataModule):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/github/Advanced-AI-on-Arm/src/lab1/cifar10_trainer.py:94\u001b[39m, in \u001b[36mCIFAR10Module.val_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mval_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\n\u001b[32m    101\u001b[39m         dataset,\n\u001b[32m    102\u001b[39m         batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m         pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    106\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/torchvision/datasets/cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/torchvision/datasets/cifar.py:139\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/torchvision/datasets/utils.py:391\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    389\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m archive = os.path.join(download_root, filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/torchvision/datasets/utils.py:130\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.URLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/torchvision/datasets/utils.py:30\u001b[39m, in \u001b[36m_urlretrieve\u001b[39m\u001b[34m(url, filename, chunk_size)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total=response.length, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     31\u001b[39m         fh.write(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      4\u001b[39m module = CIFAR10Module(\n\u001b[32m      5\u001b[39m     model=vit,\n\u001b[32m      6\u001b[39m     learning_rate=\u001b[32m1e-4\u001b[39m,\n\u001b[32m      7\u001b[39m     batch_size=\u001b[32m64\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m trainer = pl.Trainer(\n\u001b[32m     11\u001b[39m     max_epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     12\u001b[39m     callbacks=[PlotlyCallback()],\n\u001b[32m     13\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     devices=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/advanced-arm-env/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from src.lab1.cifar10_trainer import CIFAR10Module, PlotlyCallback\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "module = CIFAR10Module(\n",
    "    model=vit,                                         # the model to train \n",
    "    learning_rate=1e-4,                                # the learning rate for the optimizer\n",
    "    batch_size=128                                     # the batch size for the dataloader\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',                                 # the metric to monitor \n",
    "    mode='max',                                        # the mode to use for the monitor\n",
    "    save_top_k=1,                                      # the number of top models to save\n",
    "    save_last=True,                                    # whether to save the last model\n",
    "    save_path='./src/lab1/checkpoints'                 # the path to save the model during training \n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,                                      # the number of epochs to train for\n",
    "    callbacks=[PlotlyCallback(), checkpoint_callback],  # the callbacks to use during training\n",
    "    accelerator=\"auto\",                                 # the accelerator to use during training\n",
    "    devices=\"auto\",                                     # the devices to use during training\n",
    "    precision=16                                        # the precision to use during training\n",
    ")\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2702f",
   "metadata": {},
   "source": [
    "#### Whilst your model is training, let‚Äôs take a deeper look at the theory of quantization with a hardware perspective. \n",
    "\n",
    "To understand, why quantization is such an effective approach for reducing memory consumption and latency of neural networks, we should take a look at the fundamental operation. The matrix multiply. \n",
    "\n",
    "##### A naive approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e257c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "M, N, K = 10, 32, 64\n",
    "\n",
    "X = torch.randn(M, K) # initialize the activation matrix in memory \n",
    "W = torch.randn(K, N) # initialize the weight matrix in memory\n",
    "Y = torch.empty(M, N) # allocate the output matrix memory \n",
    "\n",
    "for m in range(M): \n",
    "    for n in range(N): \n",
    "        acc = 0 # initialise the accumulator (in a fast register)\n",
    "        for k in range(K): \n",
    "            x = X[m, k] # read the activation from memory \n",
    "            w = W[k, n] # read the weight from memory \n",
    "            acc += x * w  # perform the multiply-accumulate operation\n",
    "        Y[m, n] = acc # write the result to memory \n",
    "\n",
    "assert torch.allclose(Y, torch.matmul(X, W), rtol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60fca4",
   "metadata": {},
   "source": [
    "### Matrix Multiplication: Loop Structure and Compute Breakdown\n",
    "\n",
    "In this example, we analyze a matrix multiplication kernel with three nested loops corresponding to the dimensions **M**, **N**, and **K**:\n",
    "\n",
    "- **Outer loops** iterate over the **M** (rows of output) and **N** (columns of output) dimensions.\n",
    "- **Inner loop** iterates over the **K** dimension and performs the core computation.\n",
    "\n",
    "The innermost loop executes a **multiply-accumulate (MAC)** operation ‚Äî the fundamental unit of matrix multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "#### Each MAC Operation Consists of:\n",
    "\n",
    "- üîÅ **Two memory reads**:  \n",
    "  - Load element `x[m][k]`  \n",
    "  - Load element `w[k][n]`\n",
    "  \n",
    "- ‚úñÔ∏è **One multiplication**:  \n",
    "  - `x[m][k] * w[k][n]`\n",
    "  \n",
    "- ‚ûï **One addition**:  \n",
    "  - Accumulate into the result: `acc[m][n] += ...`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Two key factors limit the speed of matrix multiplication: (1) how fast activations and weights can be fetched from memory, and (2) how quickly multiply-accumulate operations can be performed. These are governed by **memory bandwidth** and **compute throughput**, respectively ‚Äî the latter typically measured in **TOPS** (Tera Operations Per Second). However, hardware trends consistently show that compute capabilities (TOPS/FLOPS) are improving at a significantly faster rate than memory bandwidth. As a result, **memory bandwidth is increasingly becoming the primary bottleneck** for neural network inference.\n",
    "\n",
    "This is clearly illustrated in the figure below: while the performance of floating-point operations (grey line) continues to scale rapidly, both interconnect bandwidth (blue) and system memory bandwidth (green) lag behind. Consequently, **efficient memory access patterns and compression strategies** are becoming essential to reduce latency and improve throughput.\n",
    "\n",
    "his constraint is even more critical in light of emerging [*inference-time scaling laws*](https://cdn.openai.com/papers/trading-inference-time-compute-for-adversarial-robustness-20250121_1.pdf), which suggest that large models ‚Äî particularly those used for reasoning and decision-making ‚Äî exhibit higher accuracy when more compute is allocated at inference time. Thus, optimizing memory usage is not just a systems concern, but a foundational enabler of next-generation model capabilities.\n",
    "\n",
    "![alt text](./assets/lab1/bandwidth_vs_flops.png)\n",
    "\n",
    "<center>Gholami, Amir, et al. \"AI and memory wall.\" IEEE Micro, 2024.</center>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6d008",
   "metadata": {},
   "source": [
    "##### So how can we reduce the memory bandwith load caused by \n",
    " \n",
    " ```python\n",
    "  x = X[m, k]\n",
    "  w = W[k, n]\n",
    "  ...\n",
    "  y = accum \n",
    " ```\n",
    "\n",
    "The, obviously solution is to reduce the size of the memory read, and written ( ```x```/```w```, ```y```) which can be achived by reducing the numerical precision of the weights and activations, allowing reduced bit-width representation of the data-types. In edge AI, it is common to use [*8-bit integer quantization*](https://arxiv.org/pdf/2106.08295) for weight and/or activations and often results in negligible accuracy loss, and offers a 4x reduction in memory bandwidth load over the standard fp32 precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f77ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a28270f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
