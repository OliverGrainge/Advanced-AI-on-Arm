{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e727ca7-a84c-48c2-bf0c-596f9b4f4d76",
   "metadata": {},
   "source": [
    "# üß™ Lab 2: Hardware‚ÄìSoftware Model Co-Design via Post-Training Quantization & Bit-Width Search\n",
    "\n",
    "### üìö Introduction\n",
    "\n",
    "In **Lab 1**, you saw that compressing transformer weights down to **2 bits** reduced model size by √ó16 with only a modest accuracy drop. But compression alone is a *software-centric* solution; actual deployment only succeeds when the model cooperates with the underlying silicon.\n",
    "\n",
    "In this lab, you‚Äôll adopt a **hardware‚Äìsoftware co-design** perspective, treating quantization as the critical interface between the network and its deployment hardware. Quantization affects both **model accuracy** and **execution efficiency**, making it the ideal lever for co-design.\n",
    "\n",
    "Specifically, you will:\n",
    "\n",
    "- **Wrap every `nn.Linear` in a quantized integer-only `QLinear` module**  \n",
    "- **Post-quantize both weights and activations layerwise**, selecting precision from **8 ‚Üí 2 bits**  \n",
    "- **Measure performance of each quantization configuration** using **KL-divergence** and **memory consumption**  \n",
    "- **Perform automated layerwise bit-width search** to optimize a hardware-aware objective function\n",
    "\n",
    "> **Why co-design matters:**  \n",
    "> A model that looks efficient in software may still bottleneck on real hardware due to memory access patterns, compute throughput, or unsupported bit-widths. Hardware‚Äìsoftware co-design ensures the model structure aligns with hardware constraints, enabling deployment that is both **accurate** and **efficient** on edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Lab Objectives\n",
    "\n",
    "1. **Implement `QLinear`**, a simulated integer GEMM layer with scale-offset dequantization, compatible with PyTorch CPU kernels  \n",
    "2. **Post-quantize a pretrained model checkpoint** using per-layer {8, 4, 2}-bit precision, and export metadata for downstream hardware cost modeling  \n",
    "3. **Profile** model size and **KL-divergence from the FP32 teacher model**  \n",
    "4. **Run a non-linear optimization algorithm** to identify a per-layer quantization configuration that minimizes a joint objective (accuracy vs. efficiency)\n",
    "\n",
    "---\n",
    "\n",
    "By the end, you'll produce a deployment-ready language model with a **per-layer optimal quantization configuration**‚Äîstriking the best trade-off between hardware efficiency and model fidelity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ddf624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK\n",
    "\n",
    "from src.lab1.shakespeare_trainer import ShakespeareModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724550a",
   "metadata": {},
   "source": [
    "\n",
    " ## 1Ô∏è‚É£ Building `QLinear`\n",
    "\n",
    " **Symmetric uniform quantization** maps a float tensor to signed integers\n",
    " in the range [ ‚àí(2·µá‚Åª¬π‚àí1), ‚Ä¶, + (2·µá‚Åª¬π‚àí1) ] with a single scale factor **s**.\n",
    "\n",
    " Forward pass outline:\n",
    "\n",
    " 1. **Quantize** incoming activations to ints.\n",
    " 2. **Integer GEMM** with pre-quantized weights.\n",
    " 3. **De-quantize** the accumulator by multiplying with the two scales.\n",
    " 4. Add bias (still Floating Point).\n",
    "\n",
    " The class below is written for clarity rather than raw speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eabe6859-3474-4351-a760-1568adb132a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully-connected layer with symmetric uniform quantization for weights and activations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        weight_bitwidth: int = 8,\n",
    "        act_bitwidth: int = 8,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "        self.act_bitwidth = act_bitwidth\n",
    "\n",
    "        # Buffers to hold quantized weight and quantization scale\n",
    "        self.register_buffer(\n",
    "            \"qweight\",\n",
    "            torch.zeros(out_features, in_features, dtype=torch.float32),\n",
    "        )\n",
    "        self.register_buffer(\"weight_scale\", torch.ones(1))\n",
    "\n",
    "        # Optional bias stored in float32\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros(out_features, dtype=torch.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _quantize_tensor(\n",
    "        x: torch.Tensor, bitwidth: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantize a tensor to signed integers in [-2^(b-1), 2^(b-1)-1].\n",
    "        Returns (quantized_tensor, scale).\n",
    "        \"\"\"\n",
    "        qmax = 2 ** (bitwidth - 1) - 1\n",
    "        rmax = x.abs().max()\n",
    "        scale = rmax / qmax if rmax > 0 else torch.tensor(1.0, device=x.device)\n",
    "        q = torch.clamp(torch.round(x / scale), -qmax, qmax)\n",
    "        return q, scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Quantize activations\n",
    "        qx, act_scale = self._quantize_tensor(x, self.act_bitwidth)\n",
    "\n",
    "        # 2. Integer GEMM\n",
    "        qx = qx.to(self.qweight.dtype)\n",
    "        acc = qx.matmul(self.qweight.t())\n",
    "\n",
    "        # 3. Dequantize\n",
    "        y = acc * act_scale * self.weight_scale\n",
    "\n",
    "        # 4. Add bias if present\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"in={self.in_features}, out={self.out_features}, \"\n",
    "            f\"w_bits={self.weight_bitwidth}, a_bits={self.act_bitwidth})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1889a35",
   "metadata": {},
   "source": [
    " ### üëâ Quick sanity check\n",
    "\n",
    " Run the next cell to quantize a random matrix at 2-, 4- and 8-bit and\n",
    " print the reconstruction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d289115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-bit | mean-abs-error: 0.761839\n",
      "4-bit | mean-abs-error: 0.144003\n",
      "8-bit | mean-abs-error: 0.008115\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "torch.manual_seed(0)\n",
    "sample = torch.randn(1000)\n",
    "for b in (2, 4, 8):\n",
    "    q, s = QLinear._quantize_tensor(sample, b)\n",
    "    err = (sample - q * s).abs().mean().item()\n",
    "    print(f\"{b}-bit | mean-abs-error: {err:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ab0ac",
   "metadata": {},
   "source": [
    "\n",
    " ## 2Ô∏è‚É£ Swapping Layers in-place\n",
    "\n",
    " We‚Äôll walk the model, collect string paths to every `nn.Linear`, and replace\n",
    " each with a `QLinear` whose bit-width comes from a **qconfig** dictionary:\n",
    "\n",
    " ```text\n",
    "   {\n",
    "     \"model.transformer_blocks.0.attn.q_proj\": 4,\n",
    "     \"model.transformer_blocks.0.attn.k_proj\": 2,\n",
    "     ‚Ä¶\n",
    "   }\n",
    " ```\n",
    "\n",
    " If you hand an 8-bit default config to students, they can tweak individual\n",
    " layers and re-evaluate within seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2fe4888-cc36-485a-892d-6e53903f368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî utilities for model patching\n",
    "def quantize_linear(layer: nn.Linear, weight_bitwidth=8, act_bitwidth=8):\n",
    "    qlayer = QLinear(layer.in_features, layer.out_features,\n",
    "                     bias=layer.bias is not None,\n",
    "                     weight_bitwidth=weight_bitwidth,\n",
    "                     act_bitwidth=act_bitwidth)\n",
    "    q_w, w_s = QLinear._quantize_tensor(layer.weight.data, weight_bitwidth)\n",
    "    qlayer.qweight.copy_(q_w)\n",
    "    qlayer.weight_scale.copy_(w_s)\n",
    "    if layer.bias is not None:\n",
    "        qlayer.bias.copy_(layer.bias.data)\n",
    "    return qlayer\n",
    "\n",
    "\n",
    "def quantize_model(root: nn.Module, qconfig: Dict[str, int]):\n",
    "    for path, bw in qconfig.items():\n",
    "        parent_path, _, attr = path.rpartition('.')\n",
    "        parent = root if not parent_path else root.get_submodule(parent_path)\n",
    "        setattr(parent, attr,\n",
    "                quantize_linear(getattr(parent, attr),\n",
    "                                weight_bitwidth=bw, act_bitwidth=bw))\n",
    "    return root\n",
    "\n",
    "\n",
    "def default_qconfig(model: ShakespeareModule, bitwidth=8):\n",
    "    cfg = {}\n",
    "    for name, mod in model.model.transformer_blocks.named_modules():\n",
    "        if isinstance(mod, nn.Linear):\n",
    "            cfg[f\"model.transformer_blocks.{name}\"] = bitwidth\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631f0cc",
   "metadata": {},
   "source": [
    "\n",
    " ## 3Ô∏è‚É£ Accuracy Metric: KL Divergence\n",
    "\n",
    " We can‚Äôt rely on new training loss because we haven‚Äôt re-trained.  \n",
    " Instead we measure how far the quantized logits‚Äô softmax is from the float\n",
    " model‚Äôs softmax on held-out batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b9e90f9-def3-4049-a8ee-347a7531dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî KL computation\n",
    "def compute_kl_divergence(full, quant, batches, batch_size, device):\n",
    "    dl = full.test_dataloader()\n",
    "    total = 0.0\n",
    "    full.eval(); quant.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(dl):\n",
    "            if i >= batches: break\n",
    "            x = x.to(device)\n",
    "            f_logits = full.model(x)\n",
    "            q_logits = quant.model(x)\n",
    "            kl = F.kl_div(F.log_softmax(q_logits, dim=-1),\n",
    "                          F.softmax(f_logits, dim=-1),\n",
    "                          reduction='batchmean')\n",
    "            total += kl.item()\n",
    "    return total / batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f10d4",
   "metadata": {},
   "source": [
    " ## 4Ô∏è‚É£ Memory Metric\n",
    "\n",
    " The helper below counts *every* parameter:\n",
    "\n",
    " * Quantized weights ‚Üí bitwidth from `qconfig`.\n",
    " * Everything else (biases, embeddings, layer-norm) ‚Üí 32 bits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18fc4dc5-be5a-4b1b-a2f5-c0f09165bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def compute_model_size_bytes(root: nn.Module, qconfig: Dict[str, int]):\n",
    "    total_bits = 0\n",
    "    for path, bw in qconfig.items():\n",
    "        parent_path, _, attr = path.rpartition('.')\n",
    "        parent = root if not parent_path else root.get_submodule(parent_path)\n",
    "        lin: QLinear = getattr(parent, attr)\n",
    "        total_bits += lin.qweight.numel() * bw\n",
    "        if lin.bias is not None:\n",
    "            total_bits += lin.bias.numel() * 32\n",
    "    for name, param in root.named_parameters():\n",
    "        if name.endswith('bias') or 'weight' not in name:\n",
    "            continue\n",
    "        param_module = name.rsplit('.', 1)[0]\n",
    "        if any(path.startswith(param_module) for path in qconfig):\n",
    "            continue\n",
    "        total_bits += param.numel() * 32\n",
    "    return total_bits // 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b7ee6",
   "metadata": {},
   "source": [
    "\n",
    " ## 5Ô∏è‚É£ HyperOpt Objective\n",
    "\n",
    " We scalarise two goals‚Äî**keep KL tiny, shrink size huge**‚Äîwith a single\n",
    " loss:  \n",
    " `loss = Œ± * KL + (1-Œ±) * Size_MB`.\n",
    "\n",
    " Feel free to experiment with Œ± = 0.2, 0.5, 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f3a064-fce8-4697-b112-1eca02719c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî objective\n",
    "def objective(qconfig, batches, batch_size, alpha):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    fp = ShakespeareModule.load_from_checkpoint(\n",
    "        'src/lab1/checkpoints/float-best.ckpt', batch_size=batch_size).to(device)\n",
    "    fp.setup('test')\n",
    "    q = ShakespeareModule.load_from_checkpoint(\n",
    "        'src/lab1/checkpoints/float-best.ckpt', batch_size=batch_size)\n",
    "    q.setup('test')\n",
    "    quantize_model(q, qconfig).to(device)\n",
    "\n",
    "    kl = compute_kl_divergence(fp, q, batches, batch_size, device)\n",
    "    size_mb = compute_model_size_bytes(q, qconfig) / (1024 ** 2)\n",
    "    loss = alpha * kl + (1 - alpha) * size_mb\n",
    "    #print(f\"loss={loss:.4f} | KL={kl:.4f} | size={size_mb:.2f} MB\")\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e5f95",
   "metadata": {},
   "source": [
    " ## 6Ô∏è‚É£ Search Space & Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664e042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def hyperopt_search(init_cfg, max_evals=200, batches=10, batch_size=6, alpha=1e-7):\n",
    "    space = {k: hp.choice(k, [2, 4, 8]) for k in init_cfg}\n",
    "    trials = Trials()\n",
    "    fn = partial(objective, batches=batches, batch_size=batch_size, alpha=alpha)\n",
    "    best = fmin(fn, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    return space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687ce0",
   "metadata": {},
   "source": [
    " ## 7Ô∏è‚É£ Main Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c751e464-c702-4d7c-99b7-4e38e416754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=6.6328 | KL=25.2938 | size=6.63 MB                \n",
      "loss=7.0391 | KL=6.0643 | size=7.04 MB                                           \n",
      "loss=6.6016 | KL=37.4991 | size=6.60 MB                                          \n",
      "loss=6.4453 | KL=37.7337 | size=6.45 MB                                          \n",
      "loss=6.8672 | KL=39.4967 | size=6.87 MB                                         \n",
      "loss=6.1797 | KL=46.4083 | size=6.18 MB                                         \n",
      "loss=7.1953 | KL=16.4744 | size=7.20 MB                                          \n",
      "loss=6.7734 | KL=28.7283 | size=6.77 MB                                          \n",
      "loss=6.7109 | KL=28.9668 | size=6.71 MB                                          \n",
      "loss=6.8203 | KL=16.9455 | size=6.82 MB                                          \n",
      "loss=6.4609 | KL=29.5076 | size=6.46 MB                                           \n",
      "loss=7.1797 | KL=15.7862 | size=7.18 MB                                           \n",
      "loss=5.9141 | KL=47.3078 | size=5.91 MB                                           \n",
      "loss=6.3672 | KL=41.7717 | size=6.37 MB                                           \n",
      "loss=6.6484 | KL=24.2789 | size=6.65 MB                                          \n",
      "loss=6.3516 | KL=23.7971 | size=6.35 MB                                          \n",
      "loss=6.6797 | KL=51.3631 | size=6.68 MB                                          \n",
      "loss=6.8203 | KL=10.1335 | size=6.82 MB                                          \n",
      "loss=6.2891 | KL=22.7664 | size=6.29 MB                                          \n",
      "loss=6.7734 | KL=49.3799 | size=6.77 MB                                          \n",
      "loss=5.9297 | KL=60.3116 | size=5.93 MB                                          \n",
      "loss=5.9453 | KL=51.6825 | size=5.95 MB                                          \n",
      "loss=5.8047 | KL=57.0680 | size=5.80 MB                                          \n",
      "loss=5.8516 | KL=50.3052 | size=5.85 MB                                          \n",
      "loss=5.9297 | KL=57.4210 | size=5.93 MB                                          \n",
      "loss=5.8516 | KL=51.8131 | size=5.85 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.9766 | KL=57.0369 | size=5.98 MB                                          \n",
      "loss=5.6016 | KL=77.6387 | size=5.60 MB                                          \n",
      "loss=5.6484 | KL=77.2176 | size=5.65 MB                                          \n",
      "loss=6.0234 | KL=54.4358 | size=6.02 MB                                          \n",
      "loss=5.7891 | KL=75.5651 | size=5.79 MB                                          \n",
      "loss=5.8203 | KL=99.5813 | size=5.82 MB                                          \n",
      "loss=5.7578 | KL=57.4719 | size=5.76 MB                                          \n",
      "loss=5.8516 | KL=65.0202 | size=5.85 MB                                          \n",
      "loss=5.9297 | KL=40.1659 | size=5.93 MB                                          \n",
      "loss=6.3672 | KL=44.3106 | size=6.37 MB                                          \n",
      "loss=5.9141 | KL=63.4022 | size=5.91 MB                                          \n",
      "loss=6.4297 | KL=32.4715 | size=6.43 MB                                          \n",
      "loss=5.8047 | KL=46.1951 | size=5.80 MB                                          \n",
      "loss=6.6641 | KL=49.3322 | size=6.66 MB                                          \n",
      "loss=6.0078 | KL=83.4899 | size=6.01 MB                                          \n",
      "loss=6.1953 | KL=43.3434 | size=6.20 MB                                          \n",
      "loss=5.8203 | KL=43.7445 | size=5.82 MB                                          \n",
      "loss=6.6016 | KL=29.4042 | size=6.60 MB                                          \n",
      "loss=6.3203 | KL=40.4285 | size=6.32 MB                                          \n",
      "loss=5.7422 | KL=71.8208 | size=5.74 MB                                          \n",
      "loss=6.6328 | KL=22.7515 | size=6.63 MB                                          \n",
      "loss=6.3984 | KL=42.4249 | size=6.40 MB                                          \n",
      "loss=6.1484 | KL=55.2681 | size=6.15 MB                                          \n",
      "loss=6.4453 | KL=41.1672 | size=6.45 MB                                          \n",
      "loss=6.4141 | KL=53.5662 | size=6.41 MB                                          \n",
      "loss=6.1328 | KL=57.2034 | size=6.13 MB                                          \n",
      "loss=6.5234 | KL=27.7282 | size=6.52 MB                                          \n",
      "loss=6.1797 | KL=42.6020 | size=6.18 MB                                          \n",
      "loss=6.2734 | KL=72.5602 | size=6.27 MB                                          \n",
      "loss=5.8359 | KL=62.7861 | size=5.84 MB                                          \n",
      "loss=6.4141 | KL=35.5485 | size=6.41 MB                                          \n",
      "loss=6.4609 | KL=39.9822 | size=6.46 MB                                          \n",
      "loss=6.0391 | KL=61.7057 | size=6.04 MB                                          \n",
      "loss=6.2266 | KL=44.7340 | size=6.23 MB                                          \n",
      "loss=5.7109 | KL=72.2104 | size=5.71 MB                                          \n",
      "loss=6.7266 | KL=29.6259 | size=6.73 MB                                          \n",
      "loss=6.2422 | KL=45.8870 | size=6.24 MB                                          \n",
      "loss=5.8828 | KL=54.3782 | size=5.88 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7109 | KL=56.5758 | size=5.71 MB                                          \n",
      "loss=5.7891 | KL=61.1031 | size=5.79 MB                                          \n",
      "loss=6.1328 | KL=31.2153 | size=6.13 MB                                          \n",
      "loss=6.0234 | KL=51.2187 | size=6.02 MB                                          \n",
      "loss=5.8828 | KL=52.1622 | size=5.88 MB                                          \n",
      "loss=6.4922 | KL=36.3926 | size=6.49 MB                                          \n",
      "loss=6.1797 | KL=36.7664 | size=6.18 MB                                          \n",
      "loss=6.1016 | KL=41.9251 | size=6.10 MB                                          \n",
      "loss=6.2578 | KL=55.9972 | size=6.26 MB                                          \n",
      "loss=5.9609 | KL=72.3445 | size=5.96 MB                                          \n",
      "loss=5.7891 | KL=39.8170 | size=5.79 MB                                          \n",
      "loss=6.5234 | KL=37.9491 | size=6.52 MB                                          \n",
      "loss=6.6328 | KL=33.5512 | size=6.63 MB                                          \n",
      "loss=6.3047 | KL=33.9677 | size=6.30 MB                                          \n",
      "loss=5.5547 | KL=69.3816 | size=5.55 MB                                          \n",
      "loss=6.0391 | KL=53.4676 | size=6.04 MB                                          \n",
      "loss=6.3359 | KL=42.2131 | size=6.34 MB                                          \n",
      "loss=6.2891 | KL=23.0780 | size=6.29 MB                                          \n",
      "loss=5.7422 | KL=60.4816 | size=5.74 MB                                          \n",
      "loss=6.1641 | KL=53.8182 | size=6.16 MB                                          \n",
      "loss=5.8828 | KL=75.2747 | size=5.88 MB                                          \n",
      "loss=5.6484 | KL=40.3572 | size=5.65 MB                                          \n",
      "loss=6.6172 | KL=20.8662 | size=6.62 MB                                          \n",
      "loss=6.0391 | KL=20.1830 | size=6.04 MB                                          \n",
      "loss=5.7734 | KL=42.0575 | size=5.77 MB                                          \n",
      "loss=5.7422 | KL=32.0404 | size=5.74 MB                                          \n",
      "loss=6.1797 | KL=29.8597 | size=6.18 MB                                          \n",
      "loss=5.8672 | KL=43.2547 | size=5.87 MB                                           \n",
      "loss=6.6172 | KL=20.8662 | size=6.62 MB                                           \n",
      "loss=6.0391 | KL=20.1830 | size=6.04 MB                                           \n",
      "loss=5.6484 | KL=40.3572 | size=5.65 MB                                           \n",
      "loss=5.9609 | KL=31.0930 | size=5.96 MB                                           \n",
      "loss=6.3516 | KL=30.0522 | size=6.35 MB                                           \n",
      "loss=5.9766 | KL=63.9655 | size=5.98 MB                                           \n",
      "loss=6.1172 | KL=36.7834 | size=6.12 MB                                           \n",
      "loss=6.4453 | KL=41.5096 | size=6.45 MB                                           \n",
      "loss=6.3828 | KL=23.5336 | size=6.38 MB                                           \n",
      "loss=5.8516 | KL=41.3183 | size=5.85 MB                                           \n",
      "loss=6.1016 | KL=34.8497 | size=6.10 MB                                           \n",
      "loss=6.0078 | KL=39.4739 | size=6.01 MB                                           \n",
      "loss=5.6484 | KL=115.6659 | size=5.65 MB                                          \n",
      "loss=7.1328 | KL=18.1199 | size=7.13 MB                                           \n",
      "loss=5.9922 | KL=34.4680 | size=5.99 MB                                           \n",
      "loss=5.6172 | KL=75.7411 | size=5.62 MB                                           \n",
      "loss=5.8984 | KL=39.9829 | size=5.90 MB                                           \n",
      "loss=5.7109 | KL=81.1569 | size=5.71 MB                                           \n",
      "loss=5.8359 | KL=67.0773 | size=5.84 MB                                           \n",
      "loss=6.3672 | KL=39.2364 | size=6.37 MB                                           \n",
      "loss=5.7891 | KL=67.4457 | size=5.79 MB                                           \n",
      "loss=5.6953 | KL=79.5689 | size=5.70 MB                                           \n",
      "loss=5.6328 | KL=56.3043 | size=5.63 MB                                           \n",
      "loss=6.0234 | KL=83.4014 | size=6.02 MB                                           \n",
      "loss=5.6484 | KL=68.2080 | size=5.65 MB                                           \n",
      "loss=6.3516 | KL=54.6723 | size=6.35 MB                                           \n",
      "loss=5.8047 | KL=50.3274 | size=5.80 MB                                           \n",
      "loss=5.8203 | KL=88.7264 | size=5.82 MB                                           \n",
      "loss=5.5234 | KL=95.8088 | size=5.52 MB                                           \n",
      "loss=6.6328 | KL=26.7785 | size=6.63 MB                                           \n",
      "loss=5.9453 | KL=78.6281 | size=5.95 MB                                          \n",
      "loss=6.1172 | KL=57.7665 | size=6.12 MB                                          \n",
      "loss=6.4609 | KL=37.9543 | size=6.46 MB                                          \n",
      "loss=5.6172 | KL=98.5520 | size=5.62 MB                                          \n",
      "loss=5.9141 | KL=81.3508 | size=5.91 MB                                          \n",
      "loss=6.5234 | KL=34.3529 | size=6.52 MB                                          \n",
      "loss=5.4766 | KL=83.2365 | size=5.48 MB                                          \n",
      "loss=5.4766 | KL=83.2365 | size=5.48 MB                                           \n",
      "loss=5.9766 | KL=57.1172 | size=5.98 MB                                           \n",
      "loss=5.6016 | KL=55.9796 | size=5.60 MB                                           \n",
      "loss=5.7109 | KL=80.4508 | size=5.71 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=6.1953 | KL=34.1565 | size=6.20 MB                                           \n",
      "loss=6.0078 | KL=50.6147 | size=6.01 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.4766 | KL=97.1726 | size=5.48 MB                                           \n",
      "loss=5.8984 | KL=76.1837 | size=5.90 MB                                           \n",
      "loss=5.8828 | KL=54.1952 | size=5.88 MB                                           \n",
      "loss=5.7266 | KL=66.8347 | size=5.73 MB                                           \n",
      "loss=5.8203 | KL=67.0820 | size=5.82 MB                                           \n",
      "loss=5.6641 | KL=78.1946 | size=5.66 MB                                           \n",
      "loss=6.4297 | KL=32.9938 | size=6.43 MB                                           \n",
      "loss=5.5234 | KL=87.0977 | size=5.52 MB                                           \n",
      "loss=5.8047 | KL=70.0942 | size=5.80 MB                                           \n",
      "loss=5.7734 | KL=68.2148 | size=5.77 MB                                           \n",
      "loss=5.9297 | KL=45.6614 | size=5.93 MB                                           \n",
      "loss=5.6641 | KL=78.5912 | size=5.66 MB                                           \n",
      "loss=6.0078 | KL=48.4651 | size=6.01 MB                                           \n",
      "loss=5.9453 | KL=59.9863 | size=5.95 MB                                           \n",
      "loss=5.5547 | KL=97.8683 | size=5.55 MB                                           \n",
      "loss=5.9609 | KL=59.1060 | size=5.96 MB                                           \n",
      "loss=6.0859 | KL=26.4808 | size=6.09 MB                                           \n",
      "loss=5.8047 | KL=70.4978 | size=5.80 MB                                           \n",
      "loss=6.1484 | KL=78.0652 | size=6.15 MB                                           \n",
      "loss=5.5234 | KL=78.7486 | size=5.52 MB                                           \n",
      "loss=5.7109 | KL=62.9016 | size=5.71 MB                                           \n",
      "loss=5.8203 | KL=67.0820 | size=5.82 MB                                           \n",
      "loss=6.2578 | KL=25.5271 | size=6.26 MB                                           \n",
      "loss=5.8359 | KL=75.9649 | size=5.84 MB                                           \n",
      "loss=5.7578 | KL=77.6392 | size=5.76 MB                                           \n",
      "loss=5.3672 | KL=116.2572 | size=5.37 MB                                          \n",
      "loss=6.0859 | KL=47.6167 | size=6.09 MB                                           \n",
      "loss=5.9922 | KL=87.1178 | size=5.99 MB                                           \n",
      "loss=5.8203 | KL=76.1485 | size=5.82 MB                                           \n",
      "loss=5.8516 | KL=88.8733 | size=5.85 MB                                           \n",
      "loss=5.7891 | KL=67.6307 | size=5.79 MB                                           \n",
      "loss=5.3203 | KL=113.9704 | size=5.32 MB                                          \n",
      "loss=6.0234 | KL=84.7048 | size=6.02 MB                                           \n",
      "loss=6.2109 | KL=69.5908 | size=6.21 MB                                           \n",
      "loss=6.1484 | KL=69.4217 | size=6.15 MB                                           \n",
      "loss=5.3672 | KL=105.5518 | size=5.37 MB                                          \n",
      "loss=5.6797 | KL=80.2451 | size=5.68 MB                                           \n",
      "loss=5.8047 | KL=61.1230 | size=5.80 MB                                           \n",
      "loss=5.7891 | KL=94.4477 | size=5.79 MB                                           \n",
      "loss=5.5547 | KL=98.9474 | size=5.55 MB                                           \n",
      "loss=6.1953 | KL=54.9821 | size=6.20 MB                                           \n",
      "loss=5.6641 | KL=91.1106 | size=5.66 MB                                           \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:41<00:00,  1.24trial/s, best loss: 5.320323365012666]\n",
      "Best per-layer bit-widths: {'model.transformer_blocks.0.feed_forward.linear1': 2, 'model.transformer_blocks.0.feed_forward.linear2': 4, 'model.transformer_blocks.0.self_attn.out_proj': 2, 'model.transformer_blocks.0.self_attn.qkv_proj': 4, 'model.transformer_blocks.1.feed_forward.linear1': 2, 'model.transformer_blocks.1.feed_forward.linear2': 4, 'model.transformer_blocks.1.self_attn.out_proj': 2, 'model.transformer_blocks.1.self_attn.qkv_proj': 8, 'model.transformer_blocks.2.feed_forward.linear1': 2, 'model.transformer_blocks.2.feed_forward.linear2': 2, 'model.transformer_blocks.2.self_attn.out_proj': 2, 'model.transformer_blocks.2.self_attn.qkv_proj': 4, 'model.transformer_blocks.3.feed_forward.linear1': 4, 'model.transformer_blocks.3.feed_forward.linear2': 2, 'model.transformer_blocks.3.self_attn.out_proj': 4, 'model.transformer_blocks.3.self_attn.qkv_proj': 8, 'model.transformer_blocks.4.feed_forward.linear1': 2, 'model.transformer_blocks.4.feed_forward.linear2': 2, 'model.transformer_blocks.4.self_attn.out_proj': 2, 'model.transformer_blocks.4.self_attn.qkv_proj': 2, 'model.transformer_blocks.5.feed_forward.linear1': 2, 'model.transformer_blocks.5.feed_forward.linear2': 2, 'model.transformer_blocks.5.self_attn.out_proj': 8, 'model.transformer_blocks.5.self_attn.qkv_proj': 2, 'model.transformer_blocks.6.feed_forward.linear1': 2, 'model.transformer_blocks.6.feed_forward.linear2': 2, 'model.transformer_blocks.6.self_attn.out_proj': 2, 'model.transformer_blocks.6.self_attn.qkv_proj': 2, 'model.transformer_blocks.7.feed_forward.linear1': 4, 'model.transformer_blocks.7.feed_forward.linear2': 2, 'model.transformer_blocks.7.self_attn.out_proj': 8, 'model.transformer_blocks.7.self_attn.qkv_proj': 2}\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "def main():\n",
    "    base = ShakespeareModule.load_from_checkpoint('src/lab1/checkpoints/float-best.ckpt')\n",
    "    start_cfg = default_qconfig(base, bitwidth=8)\n",
    "    best_cfg = hyperopt_search(start_cfg)\n",
    "    print(\"Best per-layer bit-widths:\", best_cfg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b412a-b517-46a4-b375-fa47c4ab4887",
   "metadata": {},
   "source": [
    "\n",
    " ## üîÑ Try This\n",
    "\n",
    " 1. **Aggressive compression** ‚Äì set the initial cfg to 4 bits and limit the\n",
    "    search to {2,4}.  How low can the KL stay?\n",
    " 2. **Latency vs throughput** ‚Äì time one forward pass before and after\n",
    "    quantization on CPU.\n",
    " 3. **Text generation side-by-side** ‚Äì sample a Shakespeare sonnet with both\n",
    "    models; can you spot the quantized one?\n",
    "\n",
    " Post your findings on the course forum‚Äîscreenshots, metrics, or even the\n",
    " strangest quantization artefacts you encounter.\n",
    "\n",
    " ---\n",
    "\n",
    " üèÅ **End of Lab 2** ‚Äî you now have a fully automated post-training\n",
    " quantization pipeline and a taste of multi-objective search.  \n",
    " Next stop: **quantization-aware training** and custom int kernels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcf5d0-350f-49d9-8044-4ac7417ba3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05740a34-df9f-44a0-9746-b26a5fbb8c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
