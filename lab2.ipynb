{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e727ca7-a84c-48c2-bf0c-596f9b4f4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lab1.shakespeare_trainer import ShakespeareModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eabe6859-3474-4351-a760-1568adb132a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = ShakespeareModule.load_from_checkpoint(\"src/lab1/checkpoints/float-best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a2fe4888-cc36-485a-892d-6e53903f368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict \n",
    "\n",
    "class QLinear(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 weight_bitwidth: int = 8,\n",
    "                 act_bitwidth: int = 8):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.has_bias = bias\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "        self.act_bitwidth = act_bitwidth\n",
    "\n",
    "        # buffers (not parameters) to hold quantized weight + scale\n",
    "        self.register_buffer(\"qweight\", torch.zeros(out_features, in_features, dtype=torch.int8))\n",
    "        self.register_buffer(\"weight_scale\", torch.ones(1))  # single scalar\n",
    "\n",
    "        # treat bias as a float32 buffer (optional)\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros(out_features, dtype=torch.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _quantize_tensor(x: torch.Tensor, bitwidth: int):\n",
    "        q_max = 2 ** (bitwidth - 1) - 1\n",
    "        r_max = x.abs().max()\n",
    "        # avoid division by zero\n",
    "        scale = r_max / q_max if r_max > 0 else torch.tensor(1.0, device=x.device)\n",
    "        q = torch.clamp(torch.round(x / scale), -q_max, +q_max).to(torch.int8)\n",
    "        return q, scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1) Quantize activations dynamically\n",
    "        q_act, act_scale = self._quantize_tensor(x, self.act_bitwidth)\n",
    "\n",
    "        # 2) Integer GEMM (q_act: int8, qweight: int8) → int32 accumulator\n",
    "        qout_int = torch.matmul(q_act, self.qweight.t().to(q_act.dtype))\n",
    "\n",
    "        # 3) Dequantize: y = qout_int * act_scale * weight_scale\n",
    "        y = qout_int.to(torch.float32) * act_scale * self.weight_scale\n",
    "\n",
    "        # 4) Add (float) bias if present\n",
    "        if self.has_bias:\n",
    "            y = y + self.bias\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __repr__(self): \n",
    "        return f\"QLinear(in_features={self.in_features}, out_features={self.out_features}, weight_bitwidth={self.weight_bitwidth})\"\n",
    "\n",
    "\n",
    "def quantize_linear(module: nn.Linear, weight_bitwidth: int = 8, act_bitwidth: int = 8) -> QLinear:\n",
    "    \"\"\"\n",
    "    Given a trained nn.Linear, return a QLinear with:\n",
    "     - weights quantized to `weight_bitwidth` bits\n",
    "     - single shared scale factor stored in module.weight_scale\n",
    "     - bias copied (fp32)\n",
    "    \"\"\"\n",
    "    # 1) instantiate QLinear with same dimensions\n",
    "    qmod = QLinear(module.in_features,\n",
    "                   module.out_features,\n",
    "                   bias=(module.bias is not None),\n",
    "                   weight_bitwidth=weight_bitwidth,\n",
    "                   act_bitwidth=act_bitwidth)\n",
    "\n",
    "    # 2) quantize the floating-point weights\n",
    "    q_w, w_scale = QLinear._quantize_tensor(module.weight.data, weight_bitwidth)\n",
    "    # copy into buffers\n",
    "    qmod.qweight.copy_(q_w)\n",
    "    qmod.weight_scale.copy_(w_scale)\n",
    "\n",
    "    # 3) copy bias (if any)\n",
    "    if module.bias is not None:\n",
    "        qmod.bias.copy_(module.bias.data)\n",
    "\n",
    "    return qmod\n",
    "\n",
    "\n",
    "def quantize_model(module: nn.Module, \n",
    "                   qconfig: Dict[str, int]) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Given an nn.Module and a qconfig dict mapping module‐paths (e.g.\n",
    "    \"model.transformer_blocks.3.attn_proj\") to weight_bitwidths,\n",
    "    replace each specified nn.Linear with a QLinear using that bitwidth\n",
    "    (and the same for activations).\n",
    "    \"\"\"\n",
    "    for full_name, weight_bw in qconfig.items():\n",
    "        # split off the last component to find the parent container\n",
    "        *parent_path, child_name = full_name.split('.')\n",
    "        \n",
    "        # navigate to the parent module\n",
    "        if parent_path:\n",
    "            parent = module.get_submodule('.'.join(parent_path))\n",
    "        else:\n",
    "            parent = module  # top‐level\n",
    "        \n",
    "        orig = getattr(parent, child_name)\n",
    "        if not isinstance(orig, nn.Linear):\n",
    "            raise ValueError(f\"Expected nn.Linear at '{full_name}', \"\n",
    "                             f\"but found {type(orig)}\")\n",
    "        \n",
    "        # quantize weight (and use same bw for activations)\n",
    "        qlin = quantize_linear(orig,\n",
    "                               weight_bitwidth=weight_bw,\n",
    "                               act_bitwidth=weight_bw)\n",
    "        \n",
    "        # overwrite in the parent\n",
    "        setattr(parent, child_name, qlin)\n",
    "    \n",
    "    return module\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b9e90f9-def3-4049-a8ee-347a7531dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShakespeareModule(\n",
      "  (model): AutoRegressiveTransformer(\n",
      "    (embedding): Embedding(1024, 256)\n",
      "    (pos_encoder): Embedding(1024, 256)\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0-7): 8 x TransformerBlock(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (qkv_proj): QLinear()\n",
      "          (out_proj): QLinear()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (linear1): QLinear()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): QLinear()\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(module)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e738bc3e-e9d3-486e-acb6-b34b67240928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_qconfig(module):\n",
    "    blocks = module.model.transformer_blocks\n",
    "    qconfig = {}\n",
    "    for name, module in blocks.named_modules(): \n",
    "        if isinstance(module, nn.Linear): \n",
    "            qconfig[f\"model.transformer_blocks.{name}\"] = 8\n",
    "    return qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18fc4dc5-be5a-4b1b-a2f5-c0f09165bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.transformer_blocks.0.self_attn.qkv_proj': 8, 'model.transformer_blocks.0.self_attn.out_proj': 8, 'model.transformer_blocks.0.feed_forward.linear1': 8, 'model.transformer_blocks.0.feed_forward.linear2': 8, 'model.transformer_blocks.1.self_attn.qkv_proj': 8, 'model.transformer_blocks.1.self_attn.out_proj': 8, 'model.transformer_blocks.1.feed_forward.linear1': 8, 'model.transformer_blocks.1.feed_forward.linear2': 8, 'model.transformer_blocks.2.self_attn.qkv_proj': 8, 'model.transformer_blocks.2.self_attn.out_proj': 8, 'model.transformer_blocks.2.feed_forward.linear1': 8, 'model.transformer_blocks.2.feed_forward.linear2': 8, 'model.transformer_blocks.3.self_attn.qkv_proj': 8, 'model.transformer_blocks.3.self_attn.out_proj': 8, 'model.transformer_blocks.3.feed_forward.linear1': 8, 'model.transformer_blocks.3.feed_forward.linear2': 8, 'model.transformer_blocks.4.self_attn.qkv_proj': 8, 'model.transformer_blocks.4.self_attn.out_proj': 8, 'model.transformer_blocks.4.feed_forward.linear1': 8, 'model.transformer_blocks.4.feed_forward.linear2': 8, 'model.transformer_blocks.5.self_attn.qkv_proj': 8, 'model.transformer_blocks.5.self_attn.out_proj': 8, 'model.transformer_blocks.5.feed_forward.linear1': 8, 'model.transformer_blocks.5.feed_forward.linear2': 8, 'model.transformer_blocks.6.self_attn.qkv_proj': 8, 'model.transformer_blocks.6.self_attn.out_proj': 8, 'model.transformer_blocks.6.feed_forward.linear1': 8, 'model.transformer_blocks.6.feed_forward.linear2': 8, 'model.transformer_blocks.7.self_attn.qkv_proj': 8, 'model.transformer_blocks.7.self_attn.out_proj': 8, 'model.transformer_blocks.7.feed_forward.linear1': 8, 'model.transformer_blocks.7.feed_forward.linear2': 8}\n"
     ]
    }
   ],
   "source": [
    "module = ShakespeareModule.load_from_checkpoint(\"src/lab1/checkpoints/float-best.ckpt\")\n",
    "qconfig = get_init_qconfig(module)\n",
    "print(qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "83f3a064-fce8-4697-b112-1eca02719c63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected nn.Linear at 'model.transformer_blocks.0.self_attn.qkv_proj', but found <class '__main__.QLinear'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m qmodule = \u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mquantize_model\u001b[39m\u001b[34m(module, qconfig)\u001b[39m\n\u001b[32m    103\u001b[39m orig = \u001b[38;5;28mgetattr\u001b[39m(parent, child_name)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig, nn.Linear):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected nn.Linear at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(orig)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# quantize weight (and use same bw for activations)\u001b[39;00m\n\u001b[32m    109\u001b[39m qlin = quantize_linear(orig,\n\u001b[32m    110\u001b[39m                        weight_bitwidth=weight_bw,\n\u001b[32m    111\u001b[39m                        act_bitwidth=weight_bw)\n",
      "\u001b[31mValueError\u001b[39m: Expected nn.Linear at 'model.transformer_blocks.0.self_attn.qkv_proj', but found <class '__main__.QLinear'>"
     ]
    }
   ],
   "source": [
    "qmodule = quantize_model(module, qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ec8fd9d-edb4-41a4-8b52-ba635f8039a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShakespeareModule(\n",
       "  (model): AutoRegressiveTransformer(\n",
       "    (embedding): Embedding(1024, 256)\n",
       "    (pos_encoder): Embedding(1024, 256)\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (qkv_proj): QLinear(in_features=256, out_features=768, weight_bitwidth=8)\n",
       "          (out_proj): QLinear(in_features=256, out_features=256, weight_bitwidth=4)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear1): QLinear(in_features=256, out_features=1024, weight_bitwidth=8)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): QLinear(in_features=1024, out_features=256, weight_bitwidth=8)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1-7): 7 x TransformerBlock(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (qkv_proj): QLinear(in_features=256, out_features=768, weight_bitwidth=8)\n",
       "          (out_proj): QLinear(in_features=256, out_features=256, weight_bitwidth=8)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (linear1): QLinear(in_features=256, out_features=1024, weight_bitwidth=8)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): QLinear(in_features=1024, out_features=256, weight_bitwidth=8)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44ae76-908d-4bd9-a5aa-f662d9d6df68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
