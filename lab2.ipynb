{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e727ca7-a84c-48c2-bf0c-596f9b4f4d76",
   "metadata": {},
   "source": [
    "# üß™ Lab 2: Hardware‚ÄìSoftware Model Co-Design via Post-Training Quantization & Bit-Width Search\n",
    "\n",
    "### üìö Introduction\n",
    "\n",
    "In **Lab 1**, you saw that compressing transformer weights down to **2 bits** reduced model size by √ó16 with only a modest accuracy drop. But compression alone is a *software-centric* solution; actual deployment only succeeds when the model cooperates with the underlying silicon.\n",
    "\n",
    "In this lab, you‚Äôll adopt a **hardware‚Äìsoftware co-design** perspective, treating quantization as the critical interface between the network and its deployment hardware. Quantization affects both **model accuracy** and **execution efficiency**, making it the ideal lever for co-design.\n",
    "\n",
    "Specifically, you will:\n",
    "\n",
    "- **Wrap every `nn.Linear` in a quantized integer-only `QLinear` module**  \n",
    "- **Post-quantize both weights and activations layerwise**, selecting precision from **8 ‚Üí 2 bits**  \n",
    "- **Measure performance of each quantization configuration** using **KL-divergence** and **memory consumption**  \n",
    "- **Perform automated layerwise bit-width search** to optimize a hardware-aware objective function\n",
    "\n",
    "> **Why co-design matters:**  \n",
    "> A model that looks efficient in software may still bottleneck on real hardware due to memory access patterns, compute throughput, or unsupported bit-widths. Hardware‚Äìsoftware co-design ensures the model structure aligns with hardware constraints, enabling deployment that is both **accurate** and **efficient** on edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Lab Objectives\n",
    "\n",
    "1. **Implement `QLinear`**, a simulated integer GEMM layer with scale-offset dequantization, compatible with PyTorch CPU kernels  \n",
    "2. **Post-quantize a pretrained model checkpoint** using per-layer {8, 4, 2}-bit precision, and export metadata for downstream hardware cost modeling  \n",
    "3. **Profile** model size and **KL-divergence from the FP32 teacher model**  \n",
    "4. **Run a non-linear optimization algorithm** to identify a per-layer quantization configuration that minimizes a joint objective (accuracy vs. efficiency)\n",
    "\n",
    "---\n",
    "\n",
    "By the end, you'll produce a deployment-ready language model with a **per-layer optimal quantization configuration**‚Äîstriking the best trade-off between hardware efficiency and model fidelity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ddf624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK\n",
    "\n",
    "from src.lab1.shakespeare_trainer import ShakespeareModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724550a",
   "metadata": {},
   "source": [
    "\n",
    " ## 1Ô∏è‚É£ Building `QLinear`\n",
    "\n",
    " **Symmetric uniform quantization** maps a float tensor to signed integers\n",
    " in the range [ ‚àí(2·µá‚Åª¬π‚àí1), ‚Ä¶, + (2·µá‚Åª¬π‚àí1) ] with a single scale factor **s**.\n",
    "\n",
    " Forward pass outline:\n",
    "\n",
    " 1. **Quantize** incoming activations to ints.\n",
    " 2. **Integer GEMM** with pre-quantized weights.\n",
    " 3. **De-quantize** the accumulator by multiplying with the two scales.\n",
    " 4. Add bias (still Floating Point).\n",
    "\n",
    " The class below is written for clarity rather than raw speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eabe6859-3474-4351-a760-1568adb132a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully-connected layer with symmetric uniform quantization for weights and activations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        weight_bitwidth: int = 8,\n",
    "        act_bitwidth: int = 8,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "        self.act_bitwidth = act_bitwidth\n",
    "\n",
    "        # Buffers to hold quantized weight and quantization scale\n",
    "        self.register_buffer(\n",
    "            \"qweight\",\n",
    "            torch.zeros(out_features, in_features, dtype=torch.float32),\n",
    "        )\n",
    "        self.register_buffer(\"weight_scale\", torch.ones(1))\n",
    "\n",
    "        # Optional bias stored in float32\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros(out_features, dtype=torch.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _quantize_tensor(\n",
    "        x: torch.Tensor, bitwidth: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantize a tensor to signed integers in [-2^(b-1), 2^(b-1)-1].\n",
    "        Returns (quantized_tensor, scale).\n",
    "        \"\"\"\n",
    "        qmax = 2 ** (bitwidth - 1) - 1\n",
    "        rmax = x.abs().max()\n",
    "        scale = rmax / qmax if rmax > 0 else torch.tensor(1.0, device=x.device)\n",
    "        q = torch.clamp(torch.round(x / scale), -qmax, qmax)\n",
    "        return q, scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Quantize activations\n",
    "        qx, act_scale = self._quantize_tensor(x, self.act_bitwidth)\n",
    "\n",
    "        # 2. Integer GEMM\n",
    "        qx = qx.to(self.qweight.dtype)\n",
    "        acc = qx.matmul(self.qweight.t())\n",
    "\n",
    "        # 3. Dequantize\n",
    "        y = acc * act_scale * self.weight_scale\n",
    "\n",
    "        # 4. Add bias if present\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"in={self.in_features}, out={self.out_features}, \"\n",
    "            f\"w_bits={self.weight_bitwidth}, a_bits={self.act_bitwidth})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1889a35",
   "metadata": {},
   "source": [
    " ### üëâ Quick sanity check\n",
    "\n",
    " Run the next cell to quantize a random matrix at 2-, 4- and 8-bit and\n",
    " print the reconstruction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d289115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-bit | mean-abs-error: 0.761839\n",
      "4-bit | mean-abs-error: 0.144003\n",
      "8-bit | mean-abs-error: 0.008115\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "torch.manual_seed(0)\n",
    "sample = torch.randn(1000)\n",
    "for b in (2, 4, 8):\n",
    "    q, s = QLinear._quantize_tensor(sample, b)\n",
    "    err = (sample - q * s).abs().mean().item()\n",
    "    print(f\"{b}-bit | mean-abs-error: {err:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ab0ac",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Swapping Layers In-Place\n",
    "\n",
    "To quantize a model with **per-layer bit-widths**, we construct a dictionary that maps each `nn.Linear` module‚Äôs fully qualified name to its desired bit-width. This dictionary‚Äîcalled the **`qconfig`**‚Äîis then used to walk the model, locate each linear layer, and replace it with a post-quantized version (`QLinear`) that uses the specified number of bits for it's weight bit-width .\n",
    "\n",
    "The `qconfig` dictionary typically looks like this:\n",
    "\n",
    "```text\n",
    "{\n",
    "  \"model.transformer_blocks.0.attn.q_proj\": 4,\n",
    "  \"model.transformer_blocks.0.attn.k_proj\": 2,\n",
    "  ...\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2fe4888-cc36-485a-892d-6e53903f368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî utilities for model patching\n",
    "def quantize_linear(layer: nn.Linear, weight_bitwidth=8, act_bitwidth=8):\n",
    "    qlayer = QLinear(layer.in_features, layer.out_features,\n",
    "                     bias=layer.bias is not None,\n",
    "                     weight_bitwidth=weight_bitwidth,\n",
    "                     act_bitwidth=act_bitwidth)\n",
    "    q_w, w_s = QLinear._quantize_tensor(layer.weight.data, weight_bitwidth)\n",
    "    qlayer.qweight.copy_(q_w)\n",
    "    qlayer.weight_scale.copy_(w_s)\n",
    "    if layer.bias is not None:\n",
    "        qlayer.bias.copy_(layer.bias.data)\n",
    "    return qlayer\n",
    "\n",
    "\n",
    "def quantize_model(root: nn.Module, qconfig: Dict[str, int]):\n",
    "    for path, bw in qconfig.items():\n",
    "        parent_path, _, attr = path.rpartition('.')\n",
    "        parent = root if not parent_path else root.get_submodule(parent_path)\n",
    "        setattr(parent, attr,\n",
    "                quantize_linear(getattr(parent, attr),\n",
    "                                weight_bitwidth=bw, act_bitwidth=bw))\n",
    "    return root\n",
    "\n",
    "\n",
    "def default_qconfig(model: ShakespeareModule, bitwidth=8):\n",
    "    cfg = {}\n",
    "    for name, mod in model.model.transformer_blocks.named_modules():\n",
    "        if isinstance(mod, nn.Linear):\n",
    "            cfg[f\"model.transformer_blocks.{name}\"] = bitwidth\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631f0cc",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Accuracy Metric: Validation Loss (Single Batch)\n",
    "\n",
    "To evaluate the accuracy of the per-layer quantized model, we compute the **cross-entropy loss** on a single held-out batch. For a vocabulary of size $V$, and model output logits $\\mathbf{z} \\in \\mathbb{R}^{B \\times V}$ (where $B$ is the batch size) and targets $y \\in \\{0, \\ldots, V-1\\}^B$, the loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{val}} = -\\frac{1}{B} \\sum_{i=1}^B \\log \\left( \\frac{e^{z_{i, y_i}}}{\\sum_{j=1}^V e^{z_{i, j}}} \\right)\n",
    "$$\n",
    "\n",
    "This is the standard cross-entropy between the predicted softmax distribution and the true target class for each sample in the batch.\n",
    "\n",
    "In PyTorch, this is computed as:\n",
    "\n",
    "```python\n",
    "F.cross_entropy(logits, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b9e90f9-def3-4049-a8ee-347a7531dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x, y = next(iter(dataloader))\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model.model(x)\n",
    "        logits = logits.view(-1, logits.size(-1))  # shape: [6*128, 1024]\n",
    "        y = y.view(-1) \n",
    "        print(\"=========\", print(type(logits), type(y), logits.shape, y.shape))\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f10d4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Memory Metric: Static Model Size\n",
    "\n",
    "To evaluate the hardware efficiency of a given quantization configuration, we compute the **static memory footprint** of the model in **bytes**. This gives a direct measure of how much storage the model will consume on disk or in memory, based on the bit-widths assigned in `qconfig`.\n",
    "\n",
    "### üßÆ What gets counted?\n",
    "\n",
    "- **Quantized weights** (e.g., `QLinear.qweight`) are stored using the **bit-width specified in `qconfig`**\n",
    "- **Biases, embeddings, and normalization layers** remain in full precision: **32 bits per parameter**\n",
    "- **All other unquantized weights** (e.g., layer norm scales or unwrapped layers) are also assumed to be **32-bit floats**\n",
    "\n",
    "Given a quantized linear layer weight tensor with shape $[m, n]$ and bit-width $b$, its contribution to memory is:\n",
    "\n",
    "$$\n",
    "\\text{Size}_{\\text{qweight}} = m \\times n \\times b \\text{ bits}\n",
    "$$\n",
    "\n",
    "Biases and full-precision parameters contribute:\n",
    "\n",
    "$$\n",
    "\\text{Size}_{\\text{fp32}} = k \\times 32 \\text{ bits}\n",
    "$$\n",
    "\n",
    "Finally, we divide the total by 8 to convert from bits to **bytes**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18fc4dc5-be5a-4b1b-a2f5-c0f09165bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def compute_model_size_bytes(root: nn.Module, qconfig: Dict[str, int]):\n",
    "    total_bits = 0\n",
    "    for path, bw in qconfig.items():\n",
    "        parent_path, _, attr = path.rpartition('.')\n",
    "        parent = root if not parent_path else root.get_submodule(parent_path)\n",
    "        lin: QLinear = getattr(parent, attr)\n",
    "        total_bits += lin.qweight.numel() * bw\n",
    "        if lin.bias is not None:\n",
    "            total_bits += lin.bias.numel() * 32\n",
    "    for name, param in root.named_parameters():\n",
    "        if name.endswith('bias') or 'weight' not in name:\n",
    "            continue\n",
    "        param_module = name.rsplit('.', 1)[0]\n",
    "        if any(path.startswith(param_module) for path in qconfig):\n",
    "            continue\n",
    "        total_bits += param.numel() * 32\n",
    "    return total_bits // 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b7ee6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ HyperOpt Objective: Balancing Accuracy and Efficiency\n",
    "\n",
    "To search for an optimal per-layer quantization configuration, we define a **scalarized objective function** that balances two competing goals:\n",
    "\n",
    "- **Accuracy**: quantified by validation loss (cross-entropy)\n",
    "- **Efficiency**: quantified by static model size in megabytes\n",
    "\n",
    "These two metrics are combined using a **weighted linear combination**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\alpha \\cdot \\mathcal{L}_{\\text{val}} + (1 - \\alpha) \\cdot \\text{Size}_{\\text{MB}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathcal{L}_{\\text{val}}$ is the cross-entropy loss of the quantized model on a held-out batch\n",
    "- $\\text{Size}_{\\text{MB}}$ is the total model size in megabytes (as computed from `qconfig`)\n",
    "- $\\alpha \\in [0, 1]$ controls the trade-off between accuracy and memory\n",
    "\n",
    "This scalarized loss is used as the **objective function** in a non-linear optimization process (e.g., Bayesian optimization, random search). The goal is to **minimize** $\\mathcal{L}_{\\text{total}}$, yielding a quantization configuration that provides the best compromise between compactness and predictive quality.\n",
    "\n",
    "You are encouraged to experiment with different values of $\\alpha$:\n",
    "- $\\alpha = 0.2$: emphasize size minimization (more aggressive compression)\n",
    "- $\\alpha = 0.8$: prioritize accuracy (conservative quantization)\n",
    "- $\\alpha = 0.5$: balanced trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83f3a064-fce8-4697-b112-1eca02719c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî objective\n",
    "def objective(qconfig, batches, batch_size, alpha):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    q = ShakespeareModule.load_from_checkpoint(\n",
    "        'src/lab1/checkpoints/float-best.ckpt', batch_size=batch_size)\n",
    "    q.setup('test')\n",
    "    quantize_model(q, qconfig).to(device)\n",
    "\n",
    "    dl = q.test_dataloader()\n",
    "    val_loss = compute_validation_loss(q, nn.CrossEntropyLoss(), dl, device)\n",
    "\n",
    "    size_mb = compute_model_size_bytes(q, qconfig) / (1024 ** 2)\n",
    "    loss = alpha * val_loss + (1 - alpha) * size_mb\n",
    "    #print(f\"loss={loss:.4f} | KL={kl:.4f} | size={size_mb:.2f} MB\")\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e5f95",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Search Space & Driver\n",
    "\n",
    "To explore quantization configurations, we define:\n",
    "\n",
    "- A **search space**: mapping each `nn.Linear` to a discrete bit-width from `[2, 4, 8]`\n",
    "- A **driver**: an optimizer that searches this space to minimize the scalarized objective\n",
    "\n",
    "### üîç Search Space\n",
    "\n",
    "Each quantizable layer gets its own entry in `qconfig`. For a model with $L$ layers, the space has size $3^L$, so exhaustive search is impractical.\n",
    "\n",
    "Example (with `hyperopt`):\n",
    "\n",
    "```python\n",
    "space = {\n",
    "    name: hp.choice(name, [2, 4, 8])\n",
    "    for name in linear_layer_names(model)\n",
    "}\n",
    "```\n",
    "\n",
    "### üöó Search Driver\n",
    "\n",
    "We use `hyperopt.fmin()` to minimize the scalarized objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\alpha \\cdot \\mathcal{L}_{\\text{val}} + (1 - \\alpha) \\cdot \\text{Size}_{\\text{MB}}\n",
    "$$\n",
    "\n",
    "The driver proposes a `qconfig`, quantizes the model, and evaluates it using:\n",
    "\n",
    "- A single-batch validation loss for accuracy\n",
    "- A static size estimate from the quantized weights and full-precision parameters\n",
    "\n",
    "By repeating this over multiple iterations, the search converges toward a bit-width configuration that balances accuracy and memory footprint.\n",
    "\n",
    "The default optimizer (`tpe.suggest`) uses a Tree-structured Parzen Estimator, but random search or other strategies can be substituted.\n",
    "\n",
    "> üîß To accelerate search: reduce model size, limit batch size, or coarsen the bit-width choices (e.g., only 4 and 8 bits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "664e042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def hyperopt_search(init_cfg, max_evals=200, batches=10, batch_size=6, alpha=1e-7):\n",
    "    space = {k: hp.choice(k, [2, 4, 8]) for k in init_cfg}\n",
    "    trials = Trials()\n",
    "    fn = partial(objective, batches=batches, batch_size=batch_size, alpha=alpha)\n",
    "    best = fmin(fn, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    return space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687ce0",
   "metadata": {},
   "source": [
    " ## 7Ô∏è‚É£ Main Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c751e464-c702-4d7c-99b7-4e38e416754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>                                 \n",
      "<class 'torch.Tensor'>                                 \n",
      "torch.Size([6, 128, 1024])                             \n",
      "torch.Size([6, 128])                                   \n",
      "=========                                              \n",
      "None                                                   \n",
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Expected target size [6, 1024], got [6, 128]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [6, 1024], got [6, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     best_cfg = hyperopt_search(start_cfg)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_cfg\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m best_qconfig = \u001b[43moptimize_qconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36moptimize_qconfig\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m base = ShakespeareModule.load_from_checkpoint(\u001b[33m'\u001b[39m\u001b[33msrc/lab1/checkpoints/float-best.ckpt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m start_cfg = default_qconfig(base, bitwidth=\u001b[32m8\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m best_cfg = \u001b[43mhyperopt_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m best_cfg\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mhyperopt_search\u001b[39m\u001b[34m(init_cfg, max_evals, batches, batch_size, alpha)\u001b[39m\n\u001b[32m      4\u001b[39m trials = Trials()\n\u001b[32m      5\u001b[39m fn = partial(objective, batches=batches, batch_size=batch_size, alpha=alpha)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m best = \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtpe\u001b[49m\u001b[43m.\u001b[49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m space_eval(space, best)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/fmin.py:540\u001b[39m, in \u001b[36mfmin\u001b[39m\u001b[34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    537\u001b[39m     fn = __objective_fmin_wrapper(fn)\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[33m\"\u001b[39m\u001b[33mfmin\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(trials_save_file):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/base.py:671\u001b[39m, in \u001b[36mTrials.fmin\u001b[39m\u001b[34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[32m    667\u001b[39m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m=\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/fmin.py:586\u001b[39m, in \u001b[36mfmin\u001b[39m\u001b[34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[39m\n\u001b[32m    583\u001b[39m rval.catch_eval_exceptions = catch_eval_exceptions\n\u001b[32m    585\u001b[39m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m \u001b[43mrval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials.trials) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/fmin.py:364\u001b[39m, in \u001b[36mFMinIter.exhaust\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    363\u001b[39m     n_done = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.trials)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m.trials.refresh()\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/fmin.py:300\u001b[39m, in \u001b[36mFMinIter.run\u001b[39m\u001b[34m(self, N, block_until_done)\u001b[39m\n\u001b[32m    297\u001b[39m     time.sleep(\u001b[38;5;28mself\u001b[39m.poll_interval_secs)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m.trials.refresh()\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trials_save_file != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/fmin.py:178\u001b[39m, in \u001b[36mFMinIter.serial_evaluate\u001b[39m\u001b[34m(self, N)\u001b[39m\n\u001b[32m    176\u001b[39m ctrl = base.Ctrl(\u001b[38;5;28mself\u001b[39m.trials, current_trial=trial)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    180\u001b[39m     logger.error(\u001b[33m\"\u001b[39m\u001b[33mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/hyperopt/base.py:892\u001b[39m, in \u001b[36mDomain.evaluate\u001b[39m\u001b[34m(self, config, ctrl, attach_attachments)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    884\u001b[39m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[32m    885\u001b[39m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[32m    887\u001b[39m     pyll_rval = pyll.rec_eval(\n\u001b[32m    888\u001b[39m         \u001b[38;5;28mself\u001b[39m.expr,\n\u001b[32m    889\u001b[39m         memo=memo,\n\u001b[32m    890\u001b[39m         print_node_on_error=\u001b[38;5;28mself\u001b[39m.rec_eval_print_node_on_error,\n\u001b[32m    891\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m     rval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np.number)):\n\u001b[32m    895\u001b[39m     dict_rval = {\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(qconfig, batches, batch_size, alpha)\u001b[39m\n\u001b[32m      7\u001b[39m quantize_model(q, qconfig).to(device)\n\u001b[32m      9\u001b[39m dl = q.test_dataloader()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m val_loss = \u001b[43mcompute_validation_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m size_mb = compute_model_size_bytes(q, qconfig) / (\u001b[32m1024\u001b[39m ** \u001b[32m2\u001b[39m)\n\u001b[32m     13\u001b[39m loss = alpha * val_loss + (\u001b[32m1\u001b[39m - alpha) * size_mb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcompute_validation_loss\u001b[39m\u001b[34m(model, loss_fn, dataloader, device)\u001b[39m\n\u001b[32m      7\u001b[39m     logits = model.model(x)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=========\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(logits), \u001b[38;5;28mtype\u001b[39m(y), logits.shape, y.shape))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:3467\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3466\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3467\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3474\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected target size [6, 1024], got [6, 128]"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "def optimize_qconfig():\n",
    "    base = ShakespeareModule.load_from_checkpoint('src/lab1/checkpoints/float-best.ckpt')\n",
    "    start_cfg = default_qconfig(base, bitwidth=8)\n",
    "    best_cfg = hyperopt_search(start_cfg)\n",
    "    return best_cfg\n",
    "\n",
    "best_qconfig = optimize_qconfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b412a-b517-46a4-b375-fa47c4ab4887",
   "metadata": {},
   "source": [
    "\n",
    " ## üîÑ Try This\n",
    "\n",
    " 1. **Aggressive compression** ‚Äì set the initial cfg to 4 bits and limit the\n",
    "    search to {2,4}.  How low can the KL stay?\n",
    " 2. **Latency vs throughput** ‚Äì time one forward pass before and after\n",
    "    quantization on CPU.\n",
    " 3. **Text generation side-by-side** ‚Äì sample a Shakespeare sonnet with both\n",
    "    models; can you spot the quantized one?\n",
    "\n",
    " Post your findings on the course forum‚Äîscreenshots, metrics, or even the\n",
    " strangest quantization artefacts you encounter.\n",
    "\n",
    " ---\n",
    "\n",
    " üèÅ **End of Lab 2** ‚Äî you now have a fully automated post-training\n",
    " quantization pipeline and a taste of multi-objective search.  \n",
    " Next stop: **quantization-aware training** and custom int kernels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebcf5d0-350f-49d9-8044-4ac7417ba3de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05740a34-df9f-44a0-9746-b26a5fbb8c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
