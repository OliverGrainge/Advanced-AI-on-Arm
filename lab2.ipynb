{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e727ca7-a84c-48c2-bf0c-596f9b4f4d76",
   "metadata": {},
   "source": [
    "# üß™ Lab 2: Hardware‚ÄìSoftware Model Co-Design via Post-Training Quantization & Bit-Width Search\n",
    "\n",
    "### üìö Introduction\n",
    "\n",
    "In **Lab 1**, you saw that compressing transformer weights down to **2 bits** reduced model size by √ó16 with only a modest accuracy drop. But compression alone is a *software-centric* solution; actual deployment only succeeds when the model cooperates with the underlying silicon.\n",
    "\n",
    "In this lab, you‚Äôll adopt a **hardware‚Äìsoftware co-design** perspective, treating quantization as the critical interface between the network and its deployment hardware. Quantization affects both **model accuracy** and **execution efficiency**, making it the ideal lever for co-design.\n",
    "\n",
    "Specifically, you will:\n",
    "\n",
    "- **Wrap every `nn.Linear` in a quantized integer-only `QLinear` module**  \n",
    "- **Post-quantize both weights and activations layerwise**, selecting precision from **8 ‚Üí 2 bits**  \n",
    "- **Measure performance of each quantization configuration** using **KL-divergence** and **memory consumption**  \n",
    "- **Perform automated layerwise bit-width search** to optimize a hardware-aware objective function\n",
    "\n",
    "> **Why co-design matters:**  \n",
    "> A model that looks efficient in software may still bottleneck on real hardware due to memory access patterns, compute throughput, or unsupported bit-widths. Hardware‚Äìsoftware co-design ensures the model structure aligns with hardware constraints, enabling deployment that is both **accurate** and **efficient** on edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Lab Objectives\n",
    "\n",
    "1. **Implement `QLinear`**, a simulated integer GEMM layer with scale-offset dequantization, compatible with PyTorch CPU kernels  \n",
    "2. **Post-quantize a pretrained model checkpoint** using per-layer {8, 4, 2}-bit precision, and export metadata for downstream hardware cost modeling  \n",
    "3. **Profile** model size and **KL-divergence from the FP32 teacher model**  \n",
    "4. **Run a non-linear optimization algorithm** to identify a per-layer quantization configuration that minimizes a joint objective (accuracy vs. efficiency)\n",
    "\n",
    "---\n",
    "\n",
    "By the end, you'll produce a deployment-ready language model with a **per-layer optimal quantization configuration**‚Äîstriking the best trade-off between hardware efficiency and model fidelity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddf624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK\n",
    "\n",
    "from src.lab1.shakespeare_trainer import ShakespeareModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724550a",
   "metadata": {},
   "source": [
    "\n",
    " ## 1Ô∏è‚É£ Building `QLinear`\n",
    "\n",
    " **Symmetric uniform quantization** maps a float tensor to signed integers\n",
    " in the range [ ‚àí(2·µá‚Åª¬π‚àí1), ‚Ä¶, + (2·µá‚Åª¬π‚àí1) ] with a single scale factor **s**.\n",
    "\n",
    " Forward pass outline:\n",
    "\n",
    " 1. **Quantize** incoming activations to ints.\n",
    " 2. **Integer GEMM** with pre-quantized weights.\n",
    " 3. **De-quantize** the accumulator by multiplying with the two scales.\n",
    " 4. Add bias (still Floating Point).\n",
    "\n",
    " The class below is written for clarity rather than raw speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabe6859-3474-4351-a760-1568adb132a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully-connected layer with symmetric uniform quantization for weights and activations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        weight_bitwidth: int = 8,\n",
    "        act_bitwidth: int = 8,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_bitwidth = weight_bitwidth\n",
    "        self.act_bitwidth = act_bitwidth\n",
    "\n",
    "        # Buffers to hold quantized weight and quantization scale\n",
    "        self.register_buffer(\n",
    "            \"qweight\",\n",
    "            torch.zeros(out_features, in_features, dtype=torch.float32),\n",
    "        )\n",
    "        self.register_buffer(\"weight_scale\", torch.ones(1))\n",
    "\n",
    "        # Optional bias stored in float32\n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", torch.zeros(out_features, dtype=torch.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _quantize_tensor(\n",
    "        x: torch.Tensor, bitwidth: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Quantize a tensor to signed integers in [-2^(b-1), 2^(b-1)-1].\n",
    "        Returns (quantized_tensor, scale).\n",
    "        \"\"\"\n",
    "        qmax = 2 ** (bitwidth - 1) - 1\n",
    "        rmax = x.abs().max()\n",
    "        scale = rmax / qmax if rmax > 0 else torch.tensor(1.0, device=x.device)\n",
    "        q = torch.clamp(torch.round(x / scale), -qmax, qmax)\n",
    "        return q, scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Quantize activations\n",
    "        qx, act_scale = self._quantize_tensor(x, self.act_bitwidth)\n",
    "\n",
    "        # 2. Integer GEMM\n",
    "        qx = qx.to(self.qweight.dtype)\n",
    "        acc = qx.matmul(self.qweight.t())\n",
    "\n",
    "        # 3. Dequantize\n",
    "        y = acc * act_scale * self.weight_scale\n",
    "\n",
    "        # 4. Add bias if present\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "\n",
    "        return y\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"in={self.in_features}, out={self.out_features}, \"\n",
    "            f\"w_bits={self.weight_bitwidth}, a_bits={self.act_bitwidth})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1889a35",
   "metadata": {},
   "source": [
    " ### üëâ Quick sanity check\n",
    "\n",
    " Run the next cell to quantize a random matrix at 2-, 4- and 8-bit and\n",
    " print the reconstruction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d289115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-bit | mean-abs-error: 0.761839\n",
      "4-bit | mean-abs-error: 0.144003\n",
      "8-bit | mean-abs-error: 0.008115\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "torch.manual_seed(0)\n",
    "sample = torch.randn(1000)\n",
    "for b in (2, 4, 8):\n",
    "    q, s = QLinear._quantize_tensor(sample, b)\n",
    "    err = (sample - q * s).abs().mean().item()\n",
    "    print(f\"{b}-bit | mean-abs-error: {err:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ab0ac",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Swapping Layers In-Place\n",
    "\n",
    "To quantize a model with **per-layer bit-widths**, we construct a dictionary that maps each `nn.Linear` module‚Äôs fully qualified name to its desired bit-width. This dictionary‚Äîcalled the **`qconfig`**‚Äîis then used to walk the model, locate each linear layer, and replace it with a post-quantized version (`QLinear`) that uses the specified number of bits for it's weight bit-width .\n",
    "\n",
    "The `qconfig` dictionary typically looks like this:\n",
    "\n",
    "```text\n",
    "{\n",
    "  \"model.transformer_blocks.0.attn.q_proj\": 4,\n",
    "  \"model.transformer_blocks.0.attn.k_proj\": 2,\n",
    "  ...\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2fe4888-cc36-485a-892d-6e53903f368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî utilities for model patching\n",
    "def quantize_linear(layer: nn.Linear, weight_bitwidth=8, act_bitwidth=8):\n",
    "    qlayer = QLinear(layer.in_features, layer.out_features,\n",
    "                     bias=layer.bias is not None,\n",
    "                     weight_bitwidth=weight_bitwidth,\n",
    "                     act_bitwidth=act_bitwidth)\n",
    "    q_w, w_s = QLinear._quantize_tensor(layer.weight.data, weight_bitwidth)\n",
    "    qlayer.qweight.copy_(q_w)\n",
    "    qlayer.weight_scale.copy_(w_s)\n",
    "    if layer.bias is not None:\n",
    "        qlayer.bias.copy_(layer.bias.data)\n",
    "    return qlayer\n",
    "\n",
    "\n",
    "def quantize_model(root: nn.Module, qconfig: Dict[str, int]):\n",
    "    for path, bw in qconfig.items():\n",
    "        parent_path, _, attr = path.rpartition('.')\n",
    "        parent = root if not parent_path else root.get_submodule(parent_path)\n",
    "        setattr(parent, attr,\n",
    "                quantize_linear(getattr(parent, attr),\n",
    "                                weight_bitwidth=bw, act_bitwidth=bw))\n",
    "    return root\n",
    "\n",
    "\n",
    "def default_qconfig(model: ShakespeareModule, bitwidth=8):\n",
    "    cfg = {}\n",
    "    for name, mod in model.model.transformer_blocks.named_modules():\n",
    "        if isinstance(mod, nn.Linear):\n",
    "            cfg[f\"model.transformer_blocks.{name}\"] = bitwidth\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631f0cc",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Accuracy Metric: Validation Loss (Single Batch)\n",
    "\n",
    "To evaluate the accuracy of the per-layer quantized model, we compute the **cross-entropy loss** on a single held-out batch. For a vocabulary of size $V$, and model output logits $\\mathbf{z} \\in \\mathbb{R}^{B \\times V}$ (where $B$ is the batch size) and targets $y \\in \\{0, \\ldots, V-1\\}^B$, the loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{val}} = -\\frac{1}{B} \\sum_{i=1}^B \\log \\left( \\frac{e^{z_{i, y_i}}}{\\sum_{j=1}^V e^{z_{i, j}}} \\right)\n",
    "$$\n",
    "\n",
    "This is the standard cross-entropy between the predicted softmax distribution and the true target class for each sample in the batch.\n",
    "\n",
    "In PyTorch, this is computed as:\n",
    "\n",
    "```python\n",
    "F.cross_entropy(logits, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b9e90f9-def3-4049-a8ee-347a7531dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_loss(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x, y = next(iter(dataloader))\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model.model(x)\n",
    "        logits = logits.view(-1, logits.size(-1))  # shape: [6*128, 1024]\n",
    "        y = y.view(-1) \n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f10d4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Memory Metric: Static Model Size\n",
    "\n",
    "To evaluate the hardware efficiency of a given quantization configuration, we compute the **static memory footprint** of the model in **bytes**. This gives a direct measure of how much storage the model will consume on disk or in memory, based on the bit-widths assigned in `qconfig`.\n",
    "\n",
    "### üßÆ What gets counted?\n",
    "\n",
    "- **Quantized weights** (e.g., `QLinear.qweight`) are stored using the **bit-width specified in `qconfig`**\n",
    "- **Biases, embeddings, and normalization layers** remain in full precision: **32 bits per parameter**\n",
    "- **All other unquantized weights** (e.g., layer norm scales or unwrapped layers) are also assumed to be **32-bit floats**\n",
    "\n",
    "Given a quantized linear layer weight tensor with shape $[m, n]$ and bit-width $b$, its contribution to memory is:\n",
    "\n",
    "$$\n",
    "\\text{Size}_{\\text{qweight}} = m \\times n \\times b \\text{ bits}\n",
    "$$\n",
    "\n",
    "Biases and full-precision parameters contribute:\n",
    "\n",
    "$$\n",
    "\\text{Size}_{\\text{fp32}} = k \\times 32 \\text{ bits}\n",
    "$$\n",
    "\n",
    "Finally, we divide the total by 8 to convert from bits to **bytes**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fc4dc5-be5a-4b1b-a2f5-c0f09165bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def compute_model_size_bytes(root: nn.Module, qconfig: Dict[str, int]):\n",
    "    total_bits = 0\n",
    "    for path, bw in qconfig.items():\n",
    "        parent_path, _, attr = path.rpartition('.')\n",
    "        parent = root if not parent_path else root.get_submodule(parent_path)\n",
    "        lin: QLinear = getattr(parent, attr)\n",
    "        total_bits += lin.qweight.numel() * bw\n",
    "        if lin.bias is not None:\n",
    "            total_bits += lin.bias.numel() * 32\n",
    "    for name, param in root.named_parameters():\n",
    "        if name.endswith('bias') or 'weight' not in name:\n",
    "            continue\n",
    "        param_module = name.rsplit('.', 1)[0]\n",
    "        if any(path.startswith(param_module) for path in qconfig):\n",
    "            continue\n",
    "        total_bits += param.numel() * 32\n",
    "    return total_bits // 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b7ee6",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ HyperOpt Objective: Balancing Accuracy and Efficiency\n",
    "\n",
    "To search for an optimal per-layer quantization configuration, we define a **scalarized objective function** that balances two competing goals:\n",
    "\n",
    "- **Accuracy**: quantified by validation loss (cross-entropy)\n",
    "- **Efficiency**: quantified by static model size in megabytes\n",
    "\n",
    "These two metrics are combined using a **weighted linear combination**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\alpha \\cdot \\mathcal{L}_{\\text{val}} + (1 - \\alpha) \\cdot \\text{Size}_{\\text{MB}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathcal{L}_{\\text{val}}$ is the cross-entropy loss of the quantized model on a held-out batch\n",
    "- $\\text{Size}_{\\text{MB}}$ is the total model size in megabytes (as computed from `qconfig`)\n",
    "- $\\alpha \\in [0, 1]$ controls the trade-off between accuracy and memory\n",
    "\n",
    "This scalarized loss is used as the **objective function** in a non-linear optimization process (e.g., Bayesian optimization, random search). The goal is to **minimize** $\\mathcal{L}_{\\text{total}}$, yielding a quantization configuration that provides the best compromise between compactness and predictive quality.\n",
    "\n",
    "You are encouraged to experiment with different values of $\\alpha$:\n",
    "- $\\alpha = 0.2$: emphasize size minimization (more aggressive compression)\n",
    "- $\\alpha = 0.8$: prioritize accuracy (conservative quantization)\n",
    "- $\\alpha = 0.5$: balanced trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f3a064-fce8-4697-b112-1eca02719c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  ‚Äî objective\n",
    "def objective(qconfig, batches, batch_size, alpha):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    q = ShakespeareModule.load_from_checkpoint(\n",
    "        'src/lab1/checkpoints/float-best.ckpt', batch_size=batch_size)\n",
    "    q.setup('test')\n",
    "    quantize_model(q, qconfig).to(device)\n",
    "\n",
    "    dl = q.test_dataloader()\n",
    "    val_loss = compute_validation_loss(q, nn.CrossEntropyLoss(), dl, device)\n",
    "\n",
    "    size_mb = compute_model_size_bytes(q, qconfig) / (1024 ** 2)\n",
    "    loss = alpha * val_loss + (1 - alpha) * size_mb\n",
    "    #print(f\"loss={loss:.4f} | KL={kl:.4f} | size={size_mb:.2f} MB\")\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e5f95",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Search Space & Driver\n",
    "\n",
    "To explore quantization configurations, we define:\n",
    "\n",
    "- A **search space**: mapping each `nn.Linear` to a discrete bit-width from `[2, 4, 8]`\n",
    "- A **driver**: an optimizer that searches this space to minimize the scalarized objective\n",
    "\n",
    "### üîç Search Space\n",
    "\n",
    "Each quantizable layer gets its own entry in `qconfig`. For a model with $L$ layers, the space has size $3^L$, so exhaustive search is impractical.\n",
    "\n",
    "Example (with `hyperopt`):\n",
    "\n",
    "```python\n",
    "space = {\n",
    "    name: hp.choice(name, [2, 4, 8])\n",
    "    for name in linear_layer_names(model)\n",
    "}\n",
    "```\n",
    "\n",
    "### üöó Search Driver\n",
    "\n",
    "We use `hyperopt.fmin()` to minimize the scalarized objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\alpha \\cdot \\mathcal{L}_{\\text{val}} + (1 - \\alpha) \\cdot \\text{Size}_{\\text{MB}}\n",
    "$$\n",
    "\n",
    "The driver proposes a `qconfig`, quantizes the model, and evaluates it using:\n",
    "\n",
    "- A single-batch validation loss for accuracy\n",
    "- A static size estimate from the quantized weights and full-precision parameters\n",
    "\n",
    "By repeating this over multiple iterations, the search converges toward a bit-width configuration that balances accuracy and memory footprint.\n",
    "\n",
    "The default optimizer (`tpe.suggest`) uses a Tree-structured Parzen Estimator, but random search or other strategies can be substituted.\n",
    "\n",
    "> üîß To accelerate search: reduce model size, limit batch size, or coarsen the bit-width choices (e.g., only 4 and 8 bits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664e042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "def hyperopt_search(init_cfg, max_evals=200, batches=10, batch_size=6, alpha=1e-7):\n",
    "    space = {k: hp.choice(k, [2, 4, 8]) for k in init_cfg}\n",
    "    trials = Trials()\n",
    "    fn = partial(objective, batches=batches, batch_size=batch_size, alpha=alpha)\n",
    "    best = fmin(fn, space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    return space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39687ce0",
   "metadata": {},
   "source": [
    " ## 7Ô∏è‚É£ Main Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c751e464-c702-4d7c-99b7-4e38e416754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/miniconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/migration/utils.py:55: The loaded checkpoint was produced with Lightning v2.5.1, which is newer than your current Lightning version: v2.1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 190/200 [01:48<00:06,  1.62trial/s, best loss: 5.6484375103188045]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/oliver/miniconda3/lib/python3.11/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "\n",
      "  File \"/home/oliver/miniconda3/lib/python3.11/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/oliver/miniconda3/lib/python3.11/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "\n",
      "  File \"/home/oliver/miniconda3/lib/python3.11/shutil.py\", line 738, in rmtree\n",
      "    onerror(os.rmdir, path, sys.exc_info())\n",
      "\n",
      "  File \"/home/oliver/miniconda3/lib/python3.11/shutil.py\", line 736, in rmtree\n",
      "    os.rmdir(path, dir_fd=dir_fd)\n",
      "\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-4aefugpa'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.76trial/s, best loss: 5.6484375103188045]\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "def optimize_qconfig():\n",
    "    base = ShakespeareModule.load_from_checkpoint('src/lab1/checkpoints/float-best.ckpt')\n",
    "    start_cfg = default_qconfig(base, bitwidth=8)\n",
    "    best_cfg = hyperopt_search(start_cfg)\n",
    "    return best_cfg\n",
    "\n",
    "best_qconfig = optimize_qconfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b412a-b517-46a4-b375-fa47c4ab4887",
   "metadata": {},
   "source": [
    "\n",
    " ## üîÑ Try This\n",
    "\n",
    " 1. **Aggressive compression** ‚Äì set the initial cfg to 4 bits and limit the\n",
    "    search to {2,4}.  How low can the KL stay?\n",
    " 2. **Latency vs throughput** ‚Äì time one forward pass before and after\n",
    "    quantization on CPU.\n",
    " 3. **Text generation side-by-side** ‚Äì sample a Shakespeare sonnet with both\n",
    "    models; can you spot the quantized one?\n",
    "\n",
    " Post your findings on the course forum‚Äîscreenshots, metrics, or even the\n",
    " strangest quantization artefacts you encounter.\n",
    "\n",
    " ---\n",
    "\n",
    " üèÅ **End of Lab 2** ‚Äî you now have a fully automated post-training\n",
    " quantization pipeline and a taste of multi-objective search.  \n",
    " Next stop: **quantization-aware training** and custom int kernels!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ebcf5d0-350f-49d9-8044-4ac7417ba3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.transformer_blocks.0.feed_forward.linear1': 2,\n",
       " 'model.transformer_blocks.0.feed_forward.linear2': 4,\n",
       " 'model.transformer_blocks.0.self_attn.out_proj': 4,\n",
       " 'model.transformer_blocks.0.self_attn.qkv_proj': 2,\n",
       " 'model.transformer_blocks.1.feed_forward.linear1': 2,\n",
       " 'model.transformer_blocks.1.feed_forward.linear2': 4,\n",
       " 'model.transformer_blocks.1.self_attn.out_proj': 4,\n",
       " 'model.transformer_blocks.1.self_attn.qkv_proj': 2,\n",
       " 'model.transformer_blocks.2.feed_forward.linear1': 8,\n",
       " 'model.transformer_blocks.2.feed_forward.linear2': 2,\n",
       " 'model.transformer_blocks.2.self_attn.out_proj': 8,\n",
       " 'model.transformer_blocks.2.self_attn.qkv_proj': 2,\n",
       " 'model.transformer_blocks.3.feed_forward.linear1': 4,\n",
       " 'model.transformer_blocks.3.feed_forward.linear2': 8,\n",
       " 'model.transformer_blocks.3.self_attn.out_proj': 2,\n",
       " 'model.transformer_blocks.3.self_attn.qkv_proj': 2,\n",
       " 'model.transformer_blocks.4.feed_forward.linear1': 2,\n",
       " 'model.transformer_blocks.4.feed_forward.linear2': 4,\n",
       " 'model.transformer_blocks.4.self_attn.out_proj': 8,\n",
       " 'model.transformer_blocks.4.self_attn.qkv_proj': 2,\n",
       " 'model.transformer_blocks.5.feed_forward.linear1': 4,\n",
       " 'model.transformer_blocks.5.feed_forward.linear2': 2,\n",
       " 'model.transformer_blocks.5.self_attn.out_proj': 2,\n",
       " 'model.transformer_blocks.5.self_attn.qkv_proj': 4,\n",
       " 'model.transformer_blocks.6.feed_forward.linear1': 2,\n",
       " 'model.transformer_blocks.6.feed_forward.linear2': 2,\n",
       " 'model.transformer_blocks.6.self_attn.out_proj': 2,\n",
       " 'model.transformer_blocks.6.self_attn.qkv_proj': 8,\n",
       " 'model.transformer_blocks.7.feed_forward.linear1': 2,\n",
       " 'model.transformer_blocks.7.feed_forward.linear2': 2,\n",
       " 'model.transformer_blocks.7.self_attn.out_proj': 4,\n",
       " 'model.transformer_blocks.7.self_attn.qkv_proj': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_qconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05740a34-df9f-44a0-9746-b26a5fbb8c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
